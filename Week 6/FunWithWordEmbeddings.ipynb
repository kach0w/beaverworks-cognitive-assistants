{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bc6bd3b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Fun With Word Embeddings\n",
    "\n",
    "Word embeddings (or \"word vectors\") are mappings from discrete word tokens (e.g., the word \"kitten\") to numerical vectors, e.g., a 200-dimensional vector of real numbers. The goal is for words that are related (such as \"scared\" and \"afraid\") to map to points that are close together in the 200-dimensional space.\n",
    "\n",
    "These continous representations for words have proven very helpful in many NLP tasks. For example, they can help deal with synonyms that would otherwise have been considered totally unrelated in the bag of words approach to representing documents.\n",
    "\n",
    "There are several approaches for finding such an embedding. One approach is to analyze the contexts that words appear in over a large corpus and then find embeddings that map words with similar contexts to similar points in the space. For example,\n",
    "\n",
    "    I am scared of dogs.\n",
    "    I am scared of bees.\n",
    "    I am afraid of dogs.\n",
    "    I am afraid of bees.\n",
    "    ...\n",
    "\n",
    "The words \"scared\" and \"afraid\" both appear in the contexts\n",
    "\n",
    "    \"I am ... of dogs\"\n",
    "\n",
    "and\n",
    "\n",
    "    \"I am ... of bees\"\n",
    "\n",
    "so it's likely that the words are related in some way. The relationship can be semantic (related to meaning) or syntactic (e.g., often occur between a determiner and a noun) In this case, \"scared\" and \"afraid\" are related semantically (similar meaning) and also syntactically (both adjectives).\n",
    "\n",
    "One really neat thing that researchers discovered is that word embeddings can be used to solve analogies, e.g., \n",
    "\n",
    "    \"puppy\" is to \"dog\" as \"kitten\" is to ?\n",
    "    \n",
    "Amazingly, this kind of puzzle can be solved by doing computations on word vectors:\n",
    "\n",
    "```python\n",
    "wv[\"kitten\"] - wv[\"puppy\"] + wv[\"dog\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b9e811",
   "metadata": {},
   "source": [
    "and finding the most similar word to the result, `wv[\"cat\"]` in this case.\n",
    "\n",
    "The reason is that the vector `(wv[\"dog\"] - wv[\"puppy\"])` represents a direction in the space the often takes the youth version of a concept to the adult version. So starting with \"kitten\" and moving in that direction winds up in an area of the space similar to \"cat\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acd4c73",
   "metadata": {},
   "source": [
    "## 0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc74d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import time\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8425d932",
   "metadata": {},
   "source": [
    "## 1 Load pre-trained GloVe embeddings using gensim library\n",
    "\n",
    "The [gensim](https://radimrehurek.com/gensim/) library is a great tool for working with word embeddings and doing other things with text (like analyzing latent topics). If you need to install gensim, try: \n",
    "\n",
    "    pip install gensim\n",
    "\n",
    "We're going to use gensim to explore some pre-trained word embeddings trained with an algorithm called [GloVe](https://nlp.stanford.edu/projects/glove/).\n",
    "\n",
    "The following code will now use gensim to load the word vectors into a variable called `glove`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b95280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import time\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cogworks_data.language import get_data_path\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "path = get_data_path(\"glove.6B.200d.txt.w2v\")\n",
    "\n",
    "t0 = time.time()\n",
    "glove = KeyedVectors.load_word2vec_format(path, binary=False)\n",
    "t1 = time.time()\n",
    "print(\"elapsed %ss\" % (t1 - t0))\n",
    "# 50d: elapsed 17.67420792579651s\n",
    "# 100d: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c5e45d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "You can get the word vector for a word (string) with the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a949f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "glove[\"word\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3711cbe5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Print out the word vector for your favorite word. Note: you can check that the word is in the 400K lowercased vocabulary with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f564912",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\"word\" in glove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deed129a",
   "metadata": {},
   "source": [
    "What's the type of the word vector (e.g. a numpy array, a tuple)? What's its shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea30d60",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "v = glove[\"rose\"]\n",
    "print(v)\n",
    "print(type(v))\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93b9722",
   "metadata": {},
   "source": [
    "It's not clear how to (or even if we can) interpret what the individual dimensions mean. But we can gain some intuition by looking at the relationships between whole word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0123ce",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## 2 Finding most similar words\n",
    "\n",
    "You can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e87a1ca",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "glove.most_similar(\"word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cf9447",
   "metadata": {},
   "source": [
    "to find the words that the model considers most similar to a specified word (according to cosine similarity). Try it out on \"funny\" and \"pencil\" and some other words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89dd463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "print(glove.most_similar(\"funny\"))\n",
    "glove.most_similar(\"pencil\")\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abb9fa8",
   "metadata": {},
   "source": [
    "What do you notice about the relationships of the similars words to the query word? Are they all the same part of speech (e.g., all adjectives or all verbs)? Are they synonyms or near synonyms? Are they all objects of the same type (e.g., all tools)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4217265",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## 3 Visualization through dimensionality reduction\n",
    "\n",
    "It's difficult to visualize high-dimensional data like the 200-dimensional GloVe embeddings. So we're going to use (truncated) Singular Value Decomposition (SVD) to reduce the dimensions down to 2, which we can then easily plot.\n",
    "\n",
    "We'll be using scikit-learn's `TruncatedSVD` implementation. When creating the object, you provide the number of desired dimensions, e.g.,\n",
    "\n",
    "```python\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "```\n",
    "\n",
    "Then you fit the dimensionality reduction model to data with:\n",
    "\n",
    "```python\n",
    "svd.fit(X_train)\n",
    "```\n",
    "\n",
    "Finally, to transform a 200-dimensional matrix (or single vector) down to 2-dimensions according to the model, you call:\n",
    "\n",
    "```python\n",
    "X_reduced = svd.transform(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8cb463",
   "metadata": {},
   "source": [
    "### Get all embeddings into a matrix\n",
    "\n",
    "First, we'll copy all of the word embeddings into a single matrix. Note: It's a little wasteful to have loaded the embeddings using gensim (which is storing them internally already) and then copying them into a numpy array in order to apply dimensionality reduction. But it was handy to use gensim for loading and for some of it's convenient lookup methods..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa0e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(glove.key_to_index)\n",
    "d = glove.vector_size\n",
    "X_glove = np.zeros((n, d))\n",
    "for i, word in enumerate(glove.key_to_index.keys()):\n",
    "    X_glove[i,:] = glove[word]\n",
    "print(X_glove.nbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6fcf35",
   "metadata": {},
   "source": [
    "### Fit `TruncatedSVD` on the `X_glove` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723af8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "t0 = time.time()\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "svd.fit(X_glove)\n",
    "t1 = time.time()\n",
    "print(\"elapsed %ss\" % (t1 - t0))\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babbe075",
   "metadata": {},
   "source": [
    "The following helper function will help us visualize word pairs in the reduced 2-dimensional version of the word embedding space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10bad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pairs(words, word_vectors, svd):\n",
    "    \"\"\" Plots pairs of words in 2D.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    words: list[str]\n",
    "        A list with an even number of words, where pairs of words have some common relationship\n",
    "        (like profession and tool), e.g., [\"carpenter\", \"hammer\", \"plumber\", \"wrench\"].\n",
    "        \n",
    "    word_vectors: KeyedVectors instance\n",
    "        A word embedding model in gensim's KeyedVectors wrapper.\n",
    "        \n",
    "    svd: TruncatedSVD instance\n",
    "        A truncated SVD instance that's already been fit (with n_components=2).\n",
    "    \"\"\"\n",
    "\n",
    "    # map specified words to 2D space\n",
    "    d = word_vectors.vector_size\n",
    "    words_temp = np.zeros((len(words), d))\n",
    "    for i, word in enumerate(words):\n",
    "        words_temp[i,:] = word_vectors[word]\n",
    "    words_2D = svd.transform(words_temp)\n",
    "\n",
    "    # plot points\n",
    "    plt.scatter(words_2D[:,0], words_2D[:,1])\n",
    "    \n",
    "    # plot labels\n",
    "    for i, txt in enumerate(words):\n",
    "        plt.annotate(txt, (words_2D[i, 0], words_2D[i, 1]))\n",
    "\n",
    "    # plot lines\n",
    "    for i in range(int(len(words)/2)):\n",
    "        plt.plot(words_2D[i*2:i*2+2,0], words_2D[i*2:i*2+2,1], linestyle='dashed', color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6d364b",
   "metadata": {},
   "source": [
    "### Visualize: Male vs Female\n",
    "\n",
    "Try plotting these pairs and then adding some more to see how consistent the relationship is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1cb55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"man\", \"woman\", \"king\", \"queen\", \"uncle\", \"aunt\", \"nephew\", \"niece\", \"brother\", \"sister\", \"sir\", \"madam\"]\n",
    "plot_pairs(words, glove, svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd963f30",
   "metadata": {},
   "source": [
    "### Visualize: Adjective vs Comparative\n",
    "\n",
    "Try plotting these pairs and then adding some more to see how consistent the relationship is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ba99c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"short\", \"shorter\", \"strong\", \"stronger\", \"good\", \"better\"]\n",
    "plot_pairs(words, glove, svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8306916",
   "metadata": {},
   "source": [
    "### Visualize: Cellular Biology Metaphors\n",
    "\n",
    "Try plotting these pairs and then adding some more to see how consistent the relationship is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf318256",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"mitochondria\", \"cell\", \"powerhouse\", \"town\"]\n",
    "plot_pairs(words, glove, svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07711123",
   "metadata": {},
   "source": [
    "## 4 Introduction to Analogies\n",
    "\n",
    "Let's try applying word embeddings to solve analogies of the form: $a$ is to $b$ as $c$ is to ?\n",
    "\n",
    "We'll exploit the directions in the embedding space by finding the closest vector to $c + (b - a)$, or equivalently $c - a + b$.\n",
    "\n",
    "A common example is: \"puppy\" is to \"dog\" as \"kitten\" is to ?\n",
    "\n",
    "This can be solved by finding the closest vector to: \"kitten\" - \"puppy\" + \"dog\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f3cce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = glove[\"car\"] - glove[\"road\"]\n",
    "glove.similar_by_vector(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59914f76",
   "metadata": {},
   "source": [
    "Note that the most similar word (other than \"dog\" itself) is \"cat\"!\n",
    "\n",
    "Now try solving: \"france\" is to \"paris\" as \"germany\" is to ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb044800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "query = glove[\"paris\"] - glove[\"france\"] + glove[\"germany\"]\n",
    "glove.similar_by_vector(query)\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2186af9",
   "metadata": {},
   "source": [
    "Note that the gensim library has convenience methods for doing analogies. For example,\n",
    "\n",
    "\"kitten\" - \"puppy\" + \"dog\"\n",
    "\n",
    "can be solved with:\n",
    "\n",
    "```python\n",
    "glove.most_similar_cosmul(positive=['kitten', 'dog'], negative=['puppy'])\n",
    "```\n",
    "\n",
    "This uses a slightly more advanced technique for solving analogies that has \"less susceptibility to one large distance dominating the calculation\". See most_similar_cosmul() documentation for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74406eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.most_similar_cosmul(positive=['kitten', 'dog'], negative=['puppy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ed90a5",
   "metadata": {},
   "source": [
    "Try experimenting with some other kinds of word relationships (e.g., plurals, ing forms, etc.)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md,ipynb",
   "notebook_metadata_filter": "nbsphinx,-kernelspec"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
