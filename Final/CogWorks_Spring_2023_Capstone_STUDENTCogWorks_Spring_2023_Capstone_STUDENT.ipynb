{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d591dbe",
   "metadata": {},
   "source": [
    "# Image Search by Caption\n",
    "\n",
    "In this notebook, we will train a linear embedding matrix that maps shape-$(512,)$ ResNet descriptor vectors of COCO images into a word-embedding space.\n",
    "This will enable us to query images based on user-submitted text.\n",
    "\n",
    "```python\n",
    ">>> searcher(\"horses on a beach\");\n",
    "```\n",
    "\n",
    "![Horses on a beach](https://user-images.githubusercontent.com/29104956/126541255-1e1353a3-cd38-4e00-a0e9-baf9370a9eb6.png)\n",
    "\n",
    "\n",
    "Image-descriptor vectors (which have already been processed for us) will be denoted by $\\vec{d}$.\n",
    "Vectors in the embedded language space will be denoted by $\\vec{w}$.\n",
    "\n",
    "That is, we want to learn the following linear encoding:\n",
    "\n",
    "\\begin{align}\n",
    "&\\begin{bmatrix}\\leftarrow & \\vec{d}_{\\mathrm{image}} & \\rightarrow \\end{bmatrix} W_{\\mathrm{embed}} = \\begin{bmatrix}\\leftarrow & \\vec{w}_{\\mathrm{image}} & \\rightarrow \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "where $\\vec{d}_{\\mathrm{image}}$ is a $512$-dimensional descriptor vector of an image, produced by a pre-trained ResNet-18 image-classification model.\n",
    "$\\vec{w}_{\\mathrm{image}}$ is a $D$-dimensional embedding of this image descriptor.\n",
    "We want this embedded vector to \"live\" in the same \"semantic space\" as word embeddings.\n",
    "\n",
    "Suppose we want to search for pictures of \"horses on a beach\".\n",
    "We can use the $D$-dimensional GloVe embedding for each word in this \"caption\", and sum these word-embeddings with weights determined by the inverse document frequency (IDF) of each word (we will discuss how these IDFs get computed later).\n",
    "Thus we can form the embedding for this caption as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{IDF(\\mathrm{horses}})\\vec{w}_{\\mathrm{horses}} + \\mathrm{IDF(\\mathrm{on}})\\vec{w}_{\\mathrm{on}} + \\mathrm{IDF(\\mathrm{a}})\\vec{w}_{\\mathrm{a}} + \\mathrm{IDF(\\mathrm{beach}})\\vec{w}_{\\mathrm{beach}} = \\vec{w}_{\\mathrm{caption}}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\vec{w}_{\\mathrm{horses}}$ is the $D$-dimensional GloVe embedding vector for the word \"horses\", and $\\mathrm{IDF(\\mathrm{horses}})$ is the inverse document-frequency for \"horses\" (a positive scalar quantity).\n",
    "\n",
    "If we have a picture depicting horses on a beach and its corresponding descriptor vector, $\\vec{d}_{\\mathrm{image}}$ (which we are given â€“ these image descriptor vectors have been pre-created for us), then we want to be able to embed the descriptor vector for that image such that an embedding vector for the caption, $\\vec{w}_{\\mathrm{caption}}$ overlaps substantially with the image's embedding, $\\vec{w}_{\\mathrm{image}}$.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{d}_{\\mathrm{\\mathrm{image}}}W_{\\mathrm{embed}} \\rightarrow \\vec{w}_{\\mathrm{image}}\\\\\n",
    "\\hat{w}_{\\mathrm{image}}\\cdot \\hat{w}_{\\mathrm{caption}} >> 0\\\\\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99964216",
   "metadata": {},
   "source": [
    "## Our Data\n",
    "\n",
    "We are going to be working with the [MSCOCO 2014 dataset](https://cocodataset.org/#home).\n",
    "This dataset consists of 82,783 images, and each image has at least five plain-text captions that describe that image.\n",
    "These images have also been processed using a pre-trained ResNet-18 classification model, such that we also have a $512$-dimensional descriptor vector, $\\vec{d}_{\\mathrm{\\mathrm{image}}}$, associated with each image, which captures the contents of that image in an abstract way.\n",
    "\n",
    "All of the pertinent data for this project is found in three data file:\n",
    "1. Images and associated captions from the MSCOCO 2014 dataset. All of this information is stored in the `data/captions_train2014.json` JSON file. A few notes about this:\n",
    "    - We won't download all of the images at once, rather we will have a URL that we can use to download any given image.\n",
    "    - Each image has associated with it at least one, but possibly more, plain-text captions that describe it.\n",
    "2. A shape-$(1, 512)$ descriptor vector, $\\vec{d}_{\\mathrm{\\mathrm{image}}}$,  for each image from the MSCOCO dataset. Each of these was produced by processing each image with a pre-trained ResNet-18 classification model. This serves as an enriched/abstract encoding for each image in the dataset. `data/resnet18_features.pkl` contains a dictionary of `image-ID -> descriptor-vector` mappings, where `image-ID` is a unique integer ID for each image in the COCO-dataset.\n",
    "There are three files that we need for this project:\n",
    "3. The GloVe-200 word embeddings for a broad vocabulary of words. This will be used to compute $D=200$-dimensional embedding vectors $\\hat{w}_{\\mathrm{caption}}$ for each caption. These are stored in `\"data/glove.6B.200d.txt.w2v\"`\n",
    "\n",
    "\n",
    "### Loading COCO Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1d914de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json \n",
    "\n",
    "# load COCO metadata\n",
    "filename = \"data/captions_train2014.json\"\n",
    "with Path(filename).open() as f:\n",
    "    coco_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c793421",
   "metadata": {},
   "source": [
    "The `\"data/captions_train2014.json\"` JSON file has two fields that we care about: \"images\" and \"annotations\".\n",
    "\n",
    "`coco_data[\"images\"]` contains a list; each entry corresponds to a distinct **image**. For example `image_info = coco_data[\"images\"][0]` stores information for the first image.\n",
    "Each such entry contains:\n",
    "- A unique integer ID for the image (`image_info[\"id\"]`)\n",
    "- The URL where you can download the image (`image_info[\"coco_url\"]`)\n",
    "- The shape of the image (`image_info[\"height\"]`, `image_info[\"width\"]`)\n",
    "\n",
    "`coco_data[\"annotations\"]` contains a list; each entry corresponds to a distinct **caption**. For example `caption_info = coco_data[\"annotations\"][0]` stores information for the first caption.\n",
    "Each such entry contains:\n",
    "- A unique integer ID for the caption (`caption_info[\"id\"]`)\n",
    "- The ID of the image that this caption is associated with (`caption_info[\"image_id\"]`)\n",
    "- The caption, stored as a string (`caption_info[\"caption\"]`)\n",
    "\n",
    "Keep in mind that there are multiple captions associated with each image. Thus there are 82,783 entries to `coco_data[\"images\"]` and 414,113 entries to `coco_data[\"annotations\"]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e5bcf3",
   "metadata": {},
   "source": [
    "#### Organizing this data\n",
    "\n",
    "You should create functionality that stores:\n",
    "- All the image IDs\n",
    "- All the caption IDs\n",
    "- Various mappings between image/caption IDs, and associating caption-IDs with captions\n",
    "   - `image-ID -> [cap-ID-1, cap-ID-2, ...]`\n",
    "   - `caption-ID -> image-ID`\n",
    "   - `caption-ID -> \"two dogs on the grass\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e065fe3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STUDENT CODE HERE\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List\n",
    "\n",
    "image_id_to_caption_ids: Dict[int, List[int]] = defaultdict(list)\n",
    "for each in coco_data['annotations']:\n",
    "    image_id_to_caption_ids[each['image_id']].append(each['id'])\n",
    "print(image_id_to_caption_ids.values())\n",
    "\n",
    "\n",
    "\n",
    "image_ids = []\n",
    "caption_ids = []\n",
    "for i in range(len(coco_data[\"annotations\"])):\n",
    "    caption_info = coco_data[\"annotations\"][i]\n",
    "    image_ids.append(caption_info[\"image_id\"])\n",
    "    caption_ids.append(caption_info[\"id\"])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88e2201",
   "metadata": {},
   "source": [
    "### Loading GloVe Embedding and Creating Embeddings of Our Captions\n",
    "First, we will load the GloVe-200 embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bde4dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "filename = \"data/glove.6B.200d.txt.w2v\"\n",
    "glove = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c79849d",
   "metadata": {},
   "source": [
    "Note that we can query `glove` like a dictionary to get the shape-(D=200,) embedding for any word:\n",
    "\n",
    "```python\n",
    ">>> glove[\"cat\"]\n",
    "array([ 0.14823  , -0.53152  , -0.25973  , -0.44095  ,  0.38555  ,\n",
    "       -0.4114   , -0.56649  , -0.024739 , -0.2788   , -0.051034 ,\n",
    "       ...\n",
    "       0.33923  , -0.071309 ,  0.33717  , -0.0037631, -0.23328  ],\n",
    "      dtype=float32)\n",
    "```\n",
    "\n",
    "Because your functionalities above should have access of all of the captions, we can compute a single embedding vector for each of our captions. Some notes on processing captions:\n",
    " - We will lowercase, remove punctuation, and tokenize any caption that we work with.\n",
    " - **We will not worry about removing stop (a.k.a glue) words from our captions**.\n",
    "\n",
    "We compute the inverse document frequency (IDF) of every term that appears in the captions, across all captions\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{IDF(t)} = \\log_{10}{\\frac{N_{\\mathrm{captions}}}{n_{t}}}\n",
    "\\end{equation}\n",
    "\n",
    "where $n_{t}$ is the number of captions that term-$t$ appears in.\n",
    "\n",
    "Each caption's embedding is created via an IDF-weighted sum of the glove-embedding for each word in the caption.\n",
    "We then normalize this vector\n",
    "E.g, if the caption was \"Horses on a beach\", then the following shape-($D=200$,) embedding would be formed via:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{IDF(\\mathrm{horses}})\\vec{w}_{\\mathrm{horses}} + \\mathrm{IDF(\\mathrm{on}})\\vec{w}_{\\mathrm{on}} + \\mathrm{IDF(\\mathrm{a}})\\vec{w}_{\\mathrm{a}} + \\mathrm{IDF(\\mathrm{beach}})\\vec{w}_{\\mathrm{beach}} = \\vec{w}_{\\mathrm{caption}}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\mathrm{norm}(\\vec{w}_{\\mathrm{caption}}) \\rightarrow \\hat{w}_{\\mathrm{caption}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8b6669",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re, string\n",
    "\n",
    "punc_regex = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "def strip_punc(corpus):\n",
    "    return punc_regex.sub('', corpus)\n",
    "\n",
    "def to_idf(vocab, counters):\n",
    "    N = len(counters)\n",
    "    nt = [sum(1 if t in counter else 0 for counter in counters) for t in vocab]\n",
    "    nt = np.array(nt, dtype=float)\n",
    "    return np.log10(N / nt)\n",
    "    \n",
    "captions = []\n",
    "for i in range(len(coco_data[\"annotations\"])):\n",
    "    caption = coco_data[\"annotations\"][i]\n",
    "    captions.append(caption[\"caption\"])\n",
    "\n",
    "idf_sums = []\n",
    "for i in range(len(captions)):\n",
    "    caption_str = captions[i]\n",
    "    captions_arr = strip_punc(caption_str).lower().split()\n",
    "    counters = Counter(captions_arr)\n",
    "    idfs = to_idf(captions_arr, counters)\n",
    "    \n",
    "    idf_sum = 0\n",
    "    for q in range(len(idfs)):\n",
    "        idf_sum += idfs[q]*glove[counters[q]] \n",
    "    idf_sums.append(idf_sum)\n",
    "\n",
    "print(idf_sums[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc50f92e",
   "metadata": {},
   "source": [
    "### Loading Image Descriptor Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27af6b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved features\n",
    "import pickle\n",
    "with Path('data/resnet18_features.pkl').open('rb') as f:\n",
    "    resnet18_features = pickle.load(f)\n",
    "image_keys = sorted(resnet18_features.keys()) # The list of image keys in ascending order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03a598e",
   "metadata": {},
   "source": [
    "`resnet18_features` is simply a dictionary that stores a $\\vec{d}_{\\mathrm{image}}$ for each image:\n",
    "\n",
    "```\n",
    "image-ID -> shape-(512,) descriptor\n",
    "```\n",
    "\n",
    "where the image-IDs correspond to those in the COCO dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4386bfcc",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "The basics of forming our training data is the following process:\n",
    "- **Separate out image IDs into distinct sets for training and validation**\n",
    "- Pick a random training image and one of its associated captions. We'll call these our \"true image\" and \"true caption\"\n",
    "- Pick a different image. We'll call this our \"confusor image\".\n",
    "    - We can also use a fancier way to find a \"harder\" confusor image (THIS IS OPTIONAL)\n",
    "        - For some set \"tournament size\", $n$ (e.g. $n=4$)...\n",
    "        - Pick $n$ random image (must all be distinct from the true image!) and pick an associated caption for each.\n",
    "        - Compare the embeddings of the good caption and the $n$ confusor captions using cosine-similarity.\n",
    "        - The confusor caption with the highest cosine-similarity is the \"hardest\" confusor. We will use the image associated with this caption as our confusor image.\n",
    "\n",
    "Thus our training and each validation data consist of triplets: `(true-caption-ID, true-image-ID, confusor-image-ID)`.\n",
    "We will use batches of these triplets to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "48fd3021",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m num \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m414113\u001b[39m)\n\u001b[0;32m      8\u001b[0m true_caption \u001b[38;5;241m=\u001b[39m coco_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannotations\u001b[39m\u001b[38;5;124m\"\u001b[39m][num]\n\u001b[1;32m---> 10\u001b[0m true_image \u001b[38;5;241m=\u001b[39m \u001b[43mcoco_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m82783\u001b[39m):\n\u001b[0;32m     13\u001b[0m     image \u001b[38;5;241m=\u001b[39m coco_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# STUDENT CODE HERE \n",
    "import random \n",
    "\n",
    "train_data = image_ids[:(int)(0.8*(len(image_ids)))]\n",
    "validation_data = image_ids[(int)(0.8*(len(image_ids))):]\n",
    "\n",
    "num = random.randint(0, 414113)\n",
    "true_caption = coco_data[\"annotations\"][num]\n",
    "\n",
    "true_image = coco_data[\"images\"][num]\n",
    "\n",
    "for i in range(82783):\n",
    "    image = coco_data[\"images\"][i]\n",
    "    if(true_caption[\"image_id\"] == image[\"image_id\"]):\n",
    "        true_image = image\n",
    "        break\n",
    "\n",
    "new_num = random.randint(0, 82783)\n",
    "if(new_num == num):\n",
    "    new_num = random.randint(0, 82783)\n",
    "\n",
    "confusor_image = coco_data[\"images\"][new_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e20711",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Our Model\n",
    "\n",
    "Our model simply consists of one matrix that maps a shape-(512,) image descriptor into a shape-(D=200,) embedded vector, and normalizes that vector.\n",
    "\n",
    "\\begin{align}\n",
    "&\\begin{bmatrix}\\leftarrow & \\vec{d}_{\\mathrm{image}} & \\rightarrow \\end{bmatrix} W_{\\mathrm{embed}} = \\begin{bmatrix}\\leftarrow & \\vec{w}_{\\mathrm{image}} & \\rightarrow \\end{bmatrix} \\\\\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "\\mathrm{norm}(\\vec{w}_{\\mathrm{image}}) \\rightarrow \\hat{w}_{\\mathrm{image}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755e07cd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Our Loss Function\n",
    "\n",
    "Recall that we have formed triplets of `(true-caption-ID, true-image-ID, confusor-image-ID)`.\n",
    "We will use these to form a triplet of embedding vectors.\n",
    "We can simply look up the embedding vector for our caption:\n",
    " - `true-caption-ID` $\\rightarrow \\hat{w}^{\\mathrm{(true)}}_{\\mathrm{caption}}$\n",
    "\n",
    "And we can retrieve the descriptor vector for both of our images\n",
    "- `true-image-ID` $\\rightarrow \\vec{d}^{\\mathrm{(true)}}_{\\mathrm{image}}$\n",
    "- `confusor-image-ID` $\\rightarrow \\vec{d}^{\\mathrm{(confusor)}}_{\\mathrm{image}}$\n",
    "\n",
    "Processing these descriptors with our model will embed them in the same $D=200$-dimensional space as our captions:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{model}(\\vec{d}^{\\mathrm{(true)}}_{\\mathrm{image}}) &= \\hat{w}^{\\mathrm{(true)}}_{\\mathrm{image}} \\\\\n",
    "\\mathrm{model}(\\vec{d}^{\\mathrm{(confusor)}}_{\\mathrm{image}}) &= \\hat{w}^{\\mathrm{(confusor)}}_{\\mathrm{image}}\\\\\n",
    "\\end{align}\n",
    "\n",
    "We want to embed our image's descriptor in a meaningful way, such that the contents of the image reflect the semantics of its captions.\n",
    "Thus we want\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{w}^{\\mathrm{(true)}}_{\\mathrm{image}} \\cdot \\hat{w}^{\\mathrm{(true)}}_{\\mathrm{caption}} > \\hat{w}^{\\mathrm{(confusor)}}_{\\mathrm{image}} \\cdot \\hat{w}^{\\mathrm{(true)}}_{\\mathrm{caption}}\n",
    "\\end{equation}\n",
    "\n",
    "We can enforce this using a margin ranking loss:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{sim}_{\\mathrm{true}} &=  \\hat{w}^{\\mathrm{(true)}}_{\\mathrm{image}} \\cdot \\hat{w}^{\\mathrm{(true)}}_{\\mathrm{caption}} \\\\\n",
    "\\mathrm{sim}_{\\mathrm{confusor}} &=  \\hat{w}^{\\mathrm{(confusor)}}_{\\mathrm{image}} \\cdot \\hat{w}^{\\mathrm{(true)}}_{\\mathrm{caption}} \\\\\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\mathscr{L}(\\mathrm{sim}_{\\mathrm{true}}, \\mathrm{sim}_{\\mathrm{confusor}}; \\Delta) = \\max(0, \\Delta - (\\mathrm{sim}_{\\mathrm{true}} - \\mathrm{sim}_{\\mathrm{confusor}}))\n",
    "\\end{align}\n",
    "\n",
    "Note that all of our dot-products are involving unit vectors, thus we are computing cosine-similarities.\n",
    "See that this loss function encourages $\\mathrm{sim}_{\\mathrm{true}}$ to be larger than $\\mathrm{sim}_{\\mathrm{confusor}}$ by at least a margin of $\\Delta$.\n",
    "\n",
    "Of course, we will be training on **batches** of triplets. MyGrad's [margin ranking loss](https://mygrad.readthedocs.io/en/latest/generated/mygrad.nnet.losses.margin_ranking_loss.html) will automatically compute the mean over the batch dimension.\n",
    "Note that [einsum](https://mygrad.readthedocs.io/en/latest/generated/mygrad.einsum.html#mygrad.einsum) can be used to take pair-wise dot products across two batches of vectors.\n",
    "E.g. `mg.einsum(\"ni,ni -> n\", a, b)` will take two shape-$(N, D)$ arrays and compute $N$ dot products between corresponding pairs of shape-($D$,) vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecda47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These imports will assist in the writing of your model class.  \n",
    "# You should use the below layer type, loss function, optimizer, and initializer\n",
    "from mynn.optimizers import SGD\n",
    "import mygrad as mg\n",
    "from mygrad.nnet.losses import margin_ranking_loss\n",
    "from mynn.layers import dense\n",
    "from mygrad.nnet.initializers import glorot_normal\n",
    "\n",
    "# Create your model class here (you can use the class structure from \n",
    "# your autoencoder word embeddings notebook for reference, \n",
    "# though they will not be exactly the same)\n",
    "\n",
    "# STUDENT CODE HERE\n",
    "\n",
    "num_epochs = #\n",
    "batch_size = #\n",
    "margin = #\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6442e28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from noggin import create_plot\n",
    "plotter, fig, ax = create_plot([\"loss\", \"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7450f1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write a function for retrieving the descriptor vectors of each \n",
    "# image in an input batch in the form of a list of image IDs\n",
    "# and a function to process your batch using your model and loss function.  \n",
    "# The expected inputs to this function can be seen in the training loop code below\n",
    "# in the input parameters of the process_batch function\n",
    "\n",
    "# STUDENT CODE HERE\n",
    "\n",
    "# Here is the actual training loop code which should save you some time.  \n",
    "# You may need to change some of the variable names to match the ones you used above.  \n",
    "# Please ask for help with this if you get stuck\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(triples_train), batch_size):\n",
    "        loss, acc = process_batch(\n",
    "            triples_train[i : i + batch_size],\n",
    "            model,\n",
    "            margin,\n",
    "            coco=coco,\n",
    "            resnet18_features=resnet18_features,\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        plotter.set_train_batch(\n",
    "            dict(loss=loss.item(), accuracy=acc),\n",
    "            batch_size=len(triples_valid[i : i + batch_size]),\n",
    "        )\n",
    "        mg.turn_memory_guarding_off()  # slightly speeds up training\n",
    "    \n",
    "    with mg.no_autodiff:\n",
    "        for i in range(0, len(triples_valid), batch_size):\n",
    "            loss, acc = process_batch(\n",
    "                triples_valid[i : i + batch_size],\n",
    "                model,\n",
    "                margin,\n",
    "                coco=coco,\n",
    "                resnet18_features=resnet18_features,\n",
    "            )\n",
    "            plotter.set_test_batch(\n",
    "                dict(loss=loss.item(), accuracy=acc),\n",
    "                batch_size=len(triples_valid[i : i + batch_size]),\n",
    "            )\n",
    "    plotter.set_train_epoch()\n",
    "    plotter.set_test_epoch()\n",
    "plotter.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d863efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this code to save your model\n",
    "with open(\"image_embed_model.npy\", \"wb\") as f:\n",
    "    np.save(f, model.dense.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02c537a",
   "metadata": {},
   "source": [
    "## Searching Our Database\n",
    "\n",
    "It is time to create a database of images that we can search through based on user-written queries.\n",
    "We will populate this database **using only images from our validation set** so that we know that the quality of our results isn't from \"overfitting\" on our data.\n",
    "\n",
    "We have trained our embedding matrix, $W_{\\mathrm{embed}}$, we can embed each of the image descriptors from our validation set into the caption semantic space.\n",
    "\n",
    "\\begin{align}\n",
    "&\\begin{bmatrix}\\leftarrow & \\vec{d}^{(image)}_1 & \\rightarrow \\\\ \\leftarrow & \\vec{d}^{(image)}_2 & \\rightarrow \\\\ \\vdots & \\vdots & \\vdots \\\\ \\leftarrow & \\vec{d}^{(image)}_{N_{val}} & \\rightarrow\\end{bmatrix} \\rightarrow \\mathrm{model(\\dots)} \\rightarrow \\begin{bmatrix}\\leftarrow & \\hat{w}^{(image)}_1 & \\rightarrow \\\\ \\leftarrow & \\hat{w}^{(image)}_2 & \\rightarrow \\\\ \\vdots & \\vdots & \\vdots \\\\ \\leftarrow & \\hat{w}^{(image)}_{N_{val}} & \\rightarrow\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "This is our \"database\" of images.\n",
    "How do we search for relevant images given a user-supplied query?\n",
    "First, we embed the query in the same way that we embedded the captions (using an IDF-weighted sum of GloVe embeddings).\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{\"horses \\; on \\; a \\; beach\"} \\rightarrow \\mathrm{IDF(\\mathrm{horses}})\\vec{w}_{\\mathrm{horses}} + \\mathrm{IDF(\\mathrm{on}})\\vec{w}_{\\mathrm{on}} + \\mathrm{IDF(\\mathrm{a}})\\vec{w}_{\\mathrm{a}} + \\mathrm{IDF(\\mathrm{beach}})\\vec{w}_{\\mathrm{beach}} \\rightarrow \\hat{w}_{\\mathrm{query}}\n",
    "\\end{equation}\n",
    "\n",
    "Then we compute the dot product of this query's embedding against all of our image embeddings in our database.\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}\\hat{w}_{\\mathrm{query}} \\cdot \\hat{w}^{(image)}_1 \\\\ \\hat{w}_{\\mathrm{query}} \\cdot \\hat{w}^{(image)}_2 \\\\ \\vdots \\\\ \\hat{w}_{\\mathrm{query}} \\cdot \\hat{w}^{(image)}_{N_{val}}\\end{bmatrix} \\rightarrow \\mathrm{top-}k\\;\\mathrm{similarity \\; scores}\n",
    "\\end{align}\n",
    "\n",
    "the top-$k$ cosine-similarities points us to the top-$k$ most relevant images to this query!\n",
    "We need image-IDs associated with these matches and then we can fetch their associated URLs from our `CocoData` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44120e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to generate your database of output 200 length vectors for each image as generated by your model\n",
    "# STUDENT CODE HERE\n",
    "\n",
    "# Write a function to determine the n most relevant images for a given input text\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daa890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, you should have everything you need to create the full pipeline from input query to output image.  \n",
    "# If you need help putting the finishing touches together, please don't hesitate to ask.  This is where things can \n",
    "# very easily get jumbled, so it's better to ask for help early than later.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaadf29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
